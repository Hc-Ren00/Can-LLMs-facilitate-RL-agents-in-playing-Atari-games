/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.
  fn()
/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/tyro/_fields.py:343: UserWarning: The field target_kl is annotated with type <class 'float'>, but the default value None has type <class 'NoneType'>. We'll try to handle this gracefully, but it may cause unexpected behavior.
  warnings.warn(
wandb: Currently logged in as: kkapoor13 (kkapoor-13). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/expt7/wandb/run-20240602_062052-091t2zwu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Pong-v4__expt7__1__1717309250
wandb: ⭐️ View project at https://wandb.ai/kkapoor-13/fixed-code
wandb: 🚀 View run at https://wandb.ai/kkapoor-13/fixed-code/runs/091t2zwu
A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)
[Powered by Stella]
/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
wandb: - 1.056 MB of 1.056 MB uploadedwandb: \ 1.056 MB of 1.056 MB uploadedwandb: | 1.056 MB of 1.056 MB uploadedwandb: / 1.056 MB of 1.056 MB uploadedwandb: - 1.056 MB of 1.056 MB uploadedwandb: \ 1.056 MB of 1.056 MB uploadedwandb: | 1.056 MB of 1.056 MB uploadedwandb: / 103.656 MB of 206.251 MB uploaded (0.007 MB deduped)wandb: - 184.971 MB of 206.393 MB uploaded (0.007 MB deduped)wandb: 
wandb: Run history:
wandb: charts/Contribution_of_KL ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                charts/SPS ▃▆██████▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁
wandb:    charts/episodic_length ▂▁▁▃▁▂▂▂▁▄▅▃▂▂▁▂▁▅▂▂▄▂▂▁▂▂▄▅▅▁▂▅▃▁▂▃▂▇▇█
wandb:    charts/episodic_return ▃▁▃▃▁▃▁▁▁▁▆▃▃▁▁▁▃▁▁▃▃▁▃▁▁▁▃█▅▁▁▃▃▁▁▁▃▃▁▃
wandb:      charts/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:        charts/test_return ▄▄▁▂▂▂▄▄▂▂▂▃▂▄▃▂▄▃▄▇▄▂▂▂▃▂▅▄█▄▇█▇▆▄▄▄▃▂▃
wandb:               global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:          losses/approx_kl ▃▄▃▁▄▆▂▂▃▃▄██▂▃▃▄▁▅▄▂▄▂▃▅▃▄▃▃▅▆▆▂▅▂▁▅▂▇▃
wandb:           losses/clipfrac ▅▃▄▁▃▃▁▁▂▃▁▃▄▂▁▃▂▂▄▄▃▃▁▂▅▃▅▁▂▂█▆▂▅▂▃▃▃▄▅
wandb:            losses/entropy ████▇█▇█▇▇▆▇▇▇▇▇▆▆▅▅▆▇▇▅▅▅▅▅▄▅▅▄▄▄▃▃▂▁▄▅
wandb: losses/explained_variance ▁▁▅▅▄▅▆▇▅▅█▅▇█▇█▂██▇███▅▇██▆███▇▅█▇▅▆▇▃▅
wandb:      losses/old_approx_kl ▅▄▄▃▃▇▅▃▄▅▅██▄▅▆▆▃▄▃▃▂▄▅▆▅▅▅▃▅▄▁▄▆▄▄▇▄▆▃
wandb:        losses/policy_loss ▅▄▄▅▂▃▁▄▃▆▃▃█▅▄▇▅▄▆▃▃▆▄▆▃▁▅▅▅▆▄▅▃▃▃▄▄▃▄▃
wandb:         losses/total_loss ▃▃█▃▂▂▃▃▂▂▂▂▂▂▁▂▂▂▃▁▁▁▁▂▁▁▂▃▃▂▂▁▁▁▂▂▁▂▂▃
wandb:         losses/value_loss ▃▃█▂▂▂▃▃▂▂▂▂▂▂▁▂▂▁▃▁▁▁▁▁▁▁▁▃▂▁▁▁▁▁▂▂▁▁▂▃
wandb: 
wandb: Run summary:
wandb: charts/Contribution_of_KL 0.0
wandb:                charts/SPS 70.0
wandb:    charts/episodic_length 1157.0
wandb:    charts/episodic_return -21.0
wandb:      charts/learning_rate 0.00025
wandb:        charts/test_return -19.0
wandb:               global_step 221283
wandb:          losses/approx_kl 0.00278
wandb:           losses/clipfrac 0.20312
wandb:            losses/entropy 1.04807
wandb: losses/explained_variance 0.60481
wandb:      losses/old_approx_kl -0.01756
wandb:        losses/policy_loss -0.01643
wandb:         losses/total_loss 0.06422
wandb:         losses/value_loss 0.18227
wandb: 
wandb: 🚀 View run Pong-v4__expt7__1__1717309250 at: https://wandb.ai/kkapoor-13/fixed-code/runs/091t2zwu
wandb: ⭐️ View project at: https://wandb.ai/kkapoor-13/fixed-code
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240602_062052-091t2zwu/logs
Traceback (most recent call last):
  File "expt7.py", line 631, in <module>
    q_network.update(mb_states[idx].unsqueeze(0),mb_actions[idx],q_value)
  File "expt7.py", line 357, in update
    self.optimizer.step()
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 120, in step
    rmsprop(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 237, in rmsprop
    func(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 366, in _multi_tensor_rmsprop
    avg = torch._foreach_sqrt(grouped_square_avgs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 
Traceback (most recent call last):
  File "expt7.py", line 631, in <module>
    q_network.update(mb_states[idx].unsqueeze(0),mb_actions[idx],q_value)
  File "expt7.py", line 357, in update
    self.optimizer.step()
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 120, in step
    rmsprop(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 237, in rmsprop
    func(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 366, in _multi_tensor_rmsprop
    avg = torch._foreach_sqrt(grouped_square_avgs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 
