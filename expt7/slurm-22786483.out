/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.
  fn()
/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/tyro/_fields.py:343: UserWarning: The field target_kl is annotated with type <class 'float'>, but the default value None has type <class 'NoneType'>. We'll try to handle this gracefully, but it may cause unexpected behavior.
  warnings.warn(
wandb: Currently logged in as: kkapoor13 (kkapoor-13). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/expt6/wandb/run-20240514_213623-6qitdarl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Pong-v4__expt6__1__1715722582
wandb: ⭐️ View project at https://wandb.ai/kkapoor-13/fixed-code
wandb: 🚀 View run at https://wandb.ai/kkapoor-13/fixed-code/runs/6qitdarl
A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)
[Powered by Stella]
/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
wandb: - 1.236 MB of 1.236 MB uploadedwandb: \ 1.236 MB of 1.236 MB uploadedwandb: | 1.236 MB of 1.236 MB uploadedwandb: / 1.236 MB of 1.236 MB uploadedwandb: - 1.236 MB of 1.236 MB uploadedwandb: \ 1.236 MB of 1.236 MB uploadedwandb: | 1.236 MB of 1.236 MB uploadedwandb: / 1.236 MB of 1.236 MB uploadedwandb: - 1.236 MB of 1.236 MB uploadedwandb: \ 1.236 MB of 1.236 MB uploadedwandb: | 1.236 MB of 1.236 MB uploadedwandb: / 1.236 MB of 1.236 MB uploadedwandb: - 650.821 MB of 1300.401 MB uploaded (0.007 MB deduped)wandb: \ 724.852 MB of 1300.493 MB uploaded (0.007 MB deduped)wandb: | 847.212 MB of 1300.493 MB uploaded (0.007 MB deduped)wandb: / 974.305 MB of 1300.493 MB uploaded (0.007 MB deduped)wandb: - 1094.087 MB of 1300.493 MB uploaded (0.007 MB deduped)wandb: \ 1216.196 MB of 1300.493 MB uploaded (0.007 MB deduped)wandb: | 1300.493 MB of 1300.493 MB uploaded (0.007 MB deduped)wandb: 
wandb: Run history:
wandb: charts/Contribution_of_KL ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                charts/SPS ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    charts/episodic_length ▇▄▇▆▃▇▅▂▂▅█▁▁▂▂▂▂▂▂▁▁▁▂▁▂▂▂▁▂▁▁▂▂▂▂▂▂▂▂▂
wandb:    charts/episodic_return █▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      charts/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:        charts/test_return ▅▇▄▂▅▆▃▅▄▅▆▆▅███▁▁█▁▁▁▁█▁█▁█▁▁▁██▁▁▁████
wandb:               global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███
wandb:          losses/approx_kl ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁
wandb:           losses/clipfrac ▂▂▂▁▁▂▁▂▂▂▄▄▃▄▁▁▁▁█▁▁▁▁▁▁▁▆▁▁▁▁▁▅▁▁▁▁▁▁▁
wandb:            losses/entropy ██▇▇▇██▇▆▇▆▆▆▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: losses/explained_variance ▃▄▃▇▆▇▄█▆▇███▇▆▆▆▅▁▃▃▃▃▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃
wandb:  losses/kickstarting_loss ███████████████████████████████▇▇▆▆▅▄▃▂▁
wandb:      losses/old_approx_kl ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁
wandb:        losses/policy_loss ▁▂▁▁▁▂▁▁▂▂▁▂▁▃▂▂▂▂█▂▂▂▂▂▂▂▇▂▂▂▂▂▇▂▂▂▂▂▂▂
wandb:         losses/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▆▁▅▁▆▅
wandb:         losses/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▆▁▅▁▆▅
wandb: 
wandb: Run summary:
wandb: charts/Contribution_of_KL 27549.0
wandb:                charts/SPS 4.0
wandb:    charts/episodic_length 1023.0
wandb:    charts/episodic_return -21.0
wandb:      charts/learning_rate 0.00023
wandb:        charts/test_return -17.0
wandb:               global_step 221250
wandb:          losses/approx_kl 0.0
wandb:           losses/clipfrac 0.0
wandb:            losses/entropy 0.0
wandb: losses/explained_variance 0.01106
wandb:  losses/kickstarting_loss -15477336064.0
wandb:      losses/old_approx_kl 0.0
wandb:        losses/policy_loss 0.0
wandb:         losses/total_loss 42290739412992.0
wandb:         losses/value_loss 84584574222336.0
wandb: 
wandb: 🚀 View run Pong-v4__expt6__1__1715722582 at: https://wandb.ai/kkapoor-13/fixed-code/runs/6qitdarl
wandb: ⭐️ View project at: https://wandb.ai/kkapoor-13/fixed-code
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240514_213623-6qitdarl/logs
Traceback (most recent call last):
  File "expt6.py", line 628, in <module>
    q_network.update(mb_states[idx].unsqueeze(0),mb_actions[idx],q_value)
  File "expt6.py", line 355, in update
    self.optimizer.step()
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 120, in step
    rmsprop(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 237, in rmsprop
    func(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 366, in _multi_tensor_rmsprop
    avg = torch._foreach_sqrt(grouped_square_avgs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 
Traceback (most recent call last):
  File "expt6.py", line 628, in <module>
    q_network.update(mb_states[idx].unsqueeze(0),mb_actions[idx],q_value)
  File "expt6.py", line 355, in update
    self.optimizer.step()
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 120, in step
    rmsprop(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 237, in rmsprop
    func(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 366, in _multi_tensor_rmsprop
    avg = torch._foreach_sqrt(grouped_square_avgs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 
