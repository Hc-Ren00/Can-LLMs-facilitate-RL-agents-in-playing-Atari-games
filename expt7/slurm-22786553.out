/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.
  fn()
/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/tyro/_fields.py:343: UserWarning: The field target_kl is annotated with type <class 'float'>, but the default value None has type <class 'NoneType'>. We'll try to handle this gracefully, but it may cause unexpected behavior.
  warnings.warn(
wandb: Currently logged in as: kkapoor13 (kkapoor-13). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/expt6/wandb/run-20240514_214608-33t9hiyb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Pong-v4__expt6__1__1715723166
wandb: â­ï¸ View project at https://wandb.ai/kkapoor-13/fixed-code
wandb: ğŸš€ View run at https://wandb.ai/kkapoor-13/fixed-code/runs/33t9hiyb
A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)
[Powered by Stella]
/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
wandb: - 1.186 MB of 1.186 MB uploadedwandb: \ 1.186 MB of 1.186 MB uploadedwandb: | 1.186 MB of 1.186 MB uploadedwandb: / 1.186 MB of 1.186 MB uploadedwandb: - 1.186 MB of 1.186 MB uploadedwandb: \ 1.186 MB of 1.186 MB uploadedwandb: | 1.186 MB of 1.186 MB uploadedwandb: / 1.186 MB of 1.186 MB uploadedwandb: - 1.186 MB of 1.186 MB uploadedwandb: \ 1.186 MB of 1.186 MB uploadedwandb: | 1.186 MB of 1.186 MB uploadedwandb: / 588.267 MB of 1177.783 MB uploaded (0.007 MB deduped)wandb: - 610.072 MB of 1177.783 MB uploaded (0.007 MB deduped)wandb: \ 729.635 MB of 1177.783 MB uploaded (0.007 MB deduped)wandb: | 854.806 MB of 1177.783 MB uploaded (0.007 MB deduped)wandb: / 961.588 MB of 1177.783 MB uploaded (0.007 MB deduped)wandb: - 1086.931 MB of 1177.783 MB uploaded (0.007 MB deduped)wandb: \ 1177.783 MB of 1177.783 MB uploaded (0.007 MB deduped)wandb: 
wandb: Run history:
wandb: charts/Contribution_of_KL â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                charts/SPS â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    charts/episodic_length â–‚â–†â–ƒâ–ƒâ–ƒâ–„â–„â–„â–‚â–†â–ƒâ–ƒâ–…â–ˆâ–„â–ˆâ–ƒâ–…â–…â–„â–†â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–
wandb:    charts/episodic_return â–â–â–â–â–â–â–â–…â–â–…â–â–â–â–â–…â–ˆâ–â–â–â–â–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      charts/learning_rate â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:        charts/test_return â–„â–…â–ƒâ–„â–„â–ƒâ–…â–‚â–…â–„â–ƒâ–†â–ƒâ–…â–‚â–‡â–‚â–†â–„â–„â–†â–‚â–„â–ˆâ–ˆâ–â–â–â–ˆâ–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–
wandb:               global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          losses/approx_kl â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–
wandb:           losses/clipfrac â–â–â–‚â–â–‚â–â–â–â–ƒâ–‚â–â–ƒâ–â–â–â–â–‚â–â–â–‚â–‚â–‚â–„â–ˆâ–â–â–â–â–â–„â–â–â–â–â–â–â–â–â–â–
wandb:            losses/entropy â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: losses/explained_variance â–‚â–ƒâ–‚â–‡â–‡â–ˆâ–„â–ˆâ–…â–„â–‡â–†â–ˆâ–†â–‡â–ƒâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–â–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚
wandb:  losses/kickstarting_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–…â–‡â–
wandb:      losses/old_approx_kl â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–
wandb:        losses/policy_loss â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:         losses/total_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–…â–„â–„â–„â–†â–ˆ
wandb:         losses/value_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–…â–„â–„â–„â–†â–ˆ
wandb: 
wandb: Run summary:
wandb: charts/Contribution_of_KL 27570.0
wandb:                charts/SPS 4.0
wandb:    charts/episodic_length 1020.0
wandb:    charts/episodic_return -21.0
wandb:      charts/learning_rate 0.00023
wandb:        charts/test_return -21.0
wandb:               global_step 221251
wandb:          losses/approx_kl 0.0
wandb:           losses/clipfrac 0.0
wandb:            losses/entropy 0.0
wandb: losses/explained_variance 0.01083
wandb:  losses/kickstarting_loss -490238432.0
wandb:      losses/old_approx_kl 0.0
wandb:        losses/policy_loss 0.0
wandb:         losses/total_loss 62586368000.0
wandb:         losses/value_loss 125270786048.0
wandb: 
wandb: ğŸš€ View run Pong-v4__expt6__1__1715723166 at: https://wandb.ai/kkapoor-13/fixed-code/runs/33t9hiyb
wandb: â­ï¸ View project at: https://wandb.ai/kkapoor-13/fixed-code
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240514_214608-33t9hiyb/logs
Traceback (most recent call last):
  File "expt6.py", line 628, in <module>
    q_network.update(mb_states[idx].unsqueeze(0),mb_actions[idx],q_value)
  File "expt6.py", line 355, in update
    self.optimizer.step()
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 120, in step
    rmsprop(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 237, in rmsprop
    func(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 366, in _multi_tensor_rmsprop
    avg = torch._foreach_sqrt(grouped_square_avgs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 
Traceback (most recent call last):
  File "expt6.py", line 628, in <module>
    q_network.update(mb_states[idx].unsqueeze(0),mb_actions[idx],q_value)
  File "expt6.py", line 355, in update
    self.optimizer.step()
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 120, in step
    rmsprop(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 237, in rmsprop
    func(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 366, in _multi_tensor_rmsprop
    avg = torch._foreach_sqrt(grouped_square_avgs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 
