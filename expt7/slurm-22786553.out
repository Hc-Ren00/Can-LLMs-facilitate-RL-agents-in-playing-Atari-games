/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.
  fn()
/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/tyro/_fields.py:343: UserWarning: The field target_kl is annotated with type <class 'float'>, but the default value None has type <class 'NoneType'>. We'll try to handle this gracefully, but it may cause unexpected behavior.
  warnings.warn(
wandb: Currently logged in as: kkapoor13 (kkapoor-13). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/expt6/wandb/run-20240514_214608-33t9hiyb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Pong-v4__expt6__1__1715723166
wandb: ⭐️ View project at https://wandb.ai/kkapoor-13/fixed-code
wandb: 🚀 View run at https://wandb.ai/kkapoor-13/fixed-code/runs/33t9hiyb
A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)
[Powered by Stella]
/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return F.conv2d(input, weight, bias, self.stride,
/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
wandb: - 1.186 MB of 1.186 MB uploadedwandb: \ 1.186 MB of 1.186 MB uploadedwandb: | 1.186 MB of 1.186 MB uploadedwandb: / 1.186 MB of 1.186 MB uploadedwandb: - 1.186 MB of 1.186 MB uploadedwandb: \ 1.186 MB of 1.186 MB uploadedwandb: | 1.186 MB of 1.186 MB uploadedwandb: / 1.186 MB of 1.186 MB uploadedwandb: - 1.186 MB of 1.186 MB uploadedwandb: \ 1.186 MB of 1.186 MB uploadedwandb: | 1.186 MB of 1.186 MB uploadedwandb: / 588.267 MB of 1177.783 MB uploaded (0.007 MB deduped)wandb: - 610.072 MB of 1177.783 MB uploaded (0.007 MB deduped)wandb: \ 729.635 MB of 1177.783 MB uploaded (0.007 MB deduped)wandb: | 854.806 MB of 1177.783 MB uploaded (0.007 MB deduped)wandb: / 961.588 MB of 1177.783 MB uploaded (0.007 MB deduped)wandb: - 1086.931 MB of 1177.783 MB uploaded (0.007 MB deduped)wandb: \ 1177.783 MB of 1177.783 MB uploaded (0.007 MB deduped)wandb: 
wandb: Run history:
wandb: charts/Contribution_of_KL ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                charts/SPS ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:    charts/episodic_length ▂▆▃▃▃▄▄▄▂▆▃▃▅█▄█▃▅▅▄▆▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁
wandb:    charts/episodic_return ▁▁▁▁▁▁▁▅▁▅▁▁▁▁▅█▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      charts/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:        charts/test_return ▄▅▃▄▄▃▅▂▅▄▃▆▃▅▂▇▂▆▄▄▆▂▄██▁▁▁█▁▁▁████▁▁▁▁
wandb:               global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:          losses/approx_kl ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁
wandb:           losses/clipfrac ▁▁▂▁▂▁▁▁▃▂▁▃▁▁▁▁▂▁▁▂▂▂▄█▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁
wandb:            losses/entropy ███████████████████▇███▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: losses/explained_variance ▂▃▂▇▇█▄█▅▄▇▆█▆▇▃▅████▇▇▁▅▄▂▂▂▂▂▂▂▂▂▃▂▂▂▂
wandb:  losses/kickstarting_loss █████████████████████████████████▇▇▇▇▅▇▁
wandb:      losses/old_approx_kl ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁
wandb:        losses/policy_loss ▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▃▂▂▂▂▂█▂▂▂▂▂▂▂▂▂▂
wandb:         losses/total_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▅▄▄▄▆█
wandb:         losses/value_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▅▄▄▄▆█
wandb: 
wandb: Run summary:
wandb: charts/Contribution_of_KL 27570.0
wandb:                charts/SPS 4.0
wandb:    charts/episodic_length 1020.0
wandb:    charts/episodic_return -21.0
wandb:      charts/learning_rate 0.00023
wandb:        charts/test_return -21.0
wandb:               global_step 221251
wandb:          losses/approx_kl 0.0
wandb:           losses/clipfrac 0.0
wandb:            losses/entropy 0.0
wandb: losses/explained_variance 0.01083
wandb:  losses/kickstarting_loss -490238432.0
wandb:      losses/old_approx_kl 0.0
wandb:        losses/policy_loss 0.0
wandb:         losses/total_loss 62586368000.0
wandb:         losses/value_loss 125270786048.0
wandb: 
wandb: 🚀 View run Pong-v4__expt6__1__1715723166 at: https://wandb.ai/kkapoor-13/fixed-code/runs/33t9hiyb
wandb: ⭐️ View project at: https://wandb.ai/kkapoor-13/fixed-code
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240514_214608-33t9hiyb/logs
Traceback (most recent call last):
  File "expt6.py", line 628, in <module>
    q_network.update(mb_states[idx].unsqueeze(0),mb_actions[idx],q_value)
  File "expt6.py", line 355, in update
    self.optimizer.step()
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 120, in step
    rmsprop(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 237, in rmsprop
    func(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 366, in _multi_tensor_rmsprop
    avg = torch._foreach_sqrt(grouped_square_avgs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 
Traceback (most recent call last):
  File "expt6.py", line 628, in <module>
    q_network.update(mb_states[idx].unsqueeze(0),mb_actions[idx],q_value)
  File "expt6.py", line 355, in update
    self.optimizer.step()
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 391, in wrapper
    out = func(*args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 120, in step
    rmsprop(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 237, in rmsprop
    func(
  File "/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py", line 366, in _multi_tensor_rmsprop
    avg = torch._foreach_sqrt(grouped_square_avgs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 
