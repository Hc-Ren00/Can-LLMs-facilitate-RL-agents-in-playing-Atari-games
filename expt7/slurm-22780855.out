/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.
  fn()
/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/tyro/_fields.py:343: UserWarning: The field target_kl is annotated with type <class 'float'>, but the default value None has type <class 'NoneType'>. We'll try to handle this gracefully, but it may cause unexpected behavior.
  warnings.warn(
wandb: Currently logged in as: kkapoor13 (kkapoor-13). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/expt6/wandb/run-20240514_144421-qhz9q5wb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Pong-v4__expt6__1__1715697859
wandb: â­ï¸ View project at https://wandb.ai/kkapoor-13/fixed-code
wandb: ğŸš€ View run at https://wandb.ai/kkapoor-13/fixed-code/runs/qhz9q5wb
A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)
[Powered by Stella]
/work/pi_dhruveshpate_umass_edu/project_9/correct_ppo/Can-LLMs-facilitate-RL-agents-in-playing-Atari-games/venv/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
wandb: - 1.459 MB of 1.459 MB uploadedwandb: \ 1.459 MB of 1.459 MB uploadedwandb: | 1.459 MB of 1.459 MB uploadedwandb: / 1.459 MB of 1.459 MB uploadedwandb: - 1.459 MB of 1.459 MB uploadedwandb: \ 1.459 MB of 1.459 MB uploadedwandb: | 1.459 MB of 1.459 MB uploadedwandb: / 1.459 MB of 1.459 MB uploadedwandb: - 367.064 MB of 735.667 MB uploadedwandb: \ 384.175 MB of 735.667 MB uploadedwandb: | 488.315 MB of 735.667 MB uploadedwandb: / 612.096 MB of 735.667 MB uploadedwandb: - 724.268 MB of 735.667 MB uploadedwandb: 
wandb: Run history:
wandb: charts/Contribution_of_KL â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                charts/SPS â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    charts/episodic_length â–…â–‚â–ˆâ–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    charts/episodic_return â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      charts/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:        charts/test_return â–„â–…â–…â–ˆâ–…â–‚â–‚â–‡â–â–‡â–â–â–â–â–â–â–‡â–â–‡â–â–â–â–â–‡â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:               global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:          losses/approx_kl â–‚â–â–â–‚â–â–â–â–‚â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:           losses/clipfrac â–‚â–‚â–â–ƒâ–‚â–‚â–‚â–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            losses/entropy â–‡â–ˆâ–†â–ˆâ–†â–†â–†â–†â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: losses/explained_variance â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ˆâ–„â–â–â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒ
wandb:  losses/kickstarting_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–†â–†â–…â–…â–…â–…â–†â–†â–…â–†â–…â–ƒâ–â–â–â–â–â–â–â–â–
wandb:      losses/old_approx_kl â–â–‚â–„â–‚â–‚â–‚â–â–„â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:        losses/policy_loss â–„â–‚â–…â–ƒâ–â–ƒâ–â–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:         losses/total_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–„â–‡â–‡â–‡â–‡â–†â–†â–ˆâ–†â–†â–‡â–†â–ƒâ–â–â–â–â–‚â–â–â–ƒâ–â–
wandb:         losses/value_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–„â–‡â–‡â–‡â–‡â–†â–†â–ˆâ–†â–†â–‡â–†â–ƒâ–‚â–â–â–â–‚â–â–â–ƒâ–â–
wandb: 
wandb: Run summary:
wandb: charts/Contribution_of_KL 32952.0
wandb:                charts/SPS 4.0
wandb:    charts/episodic_length 1008.0
wandb:    charts/episodic_return -21.0
wandb:      charts/learning_rate 0.00023
wandb:        charts/test_return -21.0
wandb:               global_step 264576
wandb:          losses/approx_kl 0.0
wandb:           losses/clipfrac 0.0
wandb:            losses/entropy 0.0
wandb: losses/explained_variance 0.00012
wandb:  losses/kickstarting_loss -2328754.75
wandb:      losses/old_approx_kl 0.0
wandb:        losses/policy_loss 0.0
wandb:         losses/total_loss 2411691.5
wandb:         losses/value_loss 5289134.0
wandb: 
wandb: ğŸš€ View run Pong-v4__expt6__1__1715697859 at: https://wandb.ai/kkapoor-13/fixed-code/runs/qhz9q5wb
wandb: â­ï¸ View project at: https://wandb.ai/kkapoor-13/fixed-code
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240514_144421-qhz9q5wb/logs
Traceback (most recent call last):
  File "expt6.py", line 582, in <module>
    _, newlogprob, entropy, newvalue, logits = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])
  File "expt6.py", line 220, in get_action_and_value
    hidden = self.network(x / 255.0)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 
Traceback (most recent call last):
  File "expt6.py", line 582, in <module>
    _, newlogprob, entropy, newvalue, logits = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])
  File "expt6.py", line 220, in get_action_and_value
    hidden = self.network(x / 255.0)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 
